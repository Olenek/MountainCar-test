{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import load_model\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class DQAgent:\n",
    "    def __init__(self, env):\n",
    "        self._env = gym.make('MountainCar-v0')\n",
    "        self._memory = []\n",
    "        self._memory_load = 0\n",
    "        self.gamma = 0.8\n",
    "        self.learning_rate = 0.0001\n",
    "        self.model = self._build_model()\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_step = 0.1\n",
    "        self.episode_length = 201\n",
    "        self.batch_size = 4\n",
    "        \n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, input_dim=2, activation='relu'))\n",
    "        model.add(Dense(48, activation='relu'))\n",
    "        model.add(Dense(3, activation='linear'))\n",
    "        model.compile(loss='mse',\n",
    "                      optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)\n",
    "        return 0\n",
    "    \n",
    "    def load_model(self, path):\n",
    "        self.model = load_model(path)\n",
    "        return 0\n",
    "    \n",
    "    def _memorize(self, state, action, reward, next_state, done):\n",
    "        self._memory.append([state, action, reward, next_state, done])\n",
    "        self._memory_load = len(self._memory)\n",
    "        \n",
    "    def _clear_memory(self):\n",
    "        self._memory = []\n",
    "        self._memory_load = len(self._memory)\n",
    "        \n",
    "    def choose_action(self, state):\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(0, 3)\n",
    "        \n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "    \n",
    "    \n",
    "    def _replay(self):\n",
    "        if self.batch_size > self._memory_load:\n",
    "            return\n",
    "        batch = np.array(random.sample(self._memory, self.batch_size), dtype=object)\n",
    "        states, actions, rewards, next_states, dones = np.hsplit(batch, 5)\n",
    "        states = np.concatenate((np.squeeze(states[:])), axis=0)\n",
    "        actions = actions.reshape(self.batch_size,).astype(int)\n",
    "        rewards = rewards.reshape(self.batch_size,).astype(float)\n",
    "        next_states = np.concatenate(np.concatenate(next_states))\n",
    "        dones = np.concatenate(dones).astype(bool)\n",
    "        undones = ~ dones\n",
    "        undones = undones.astype(float)\n",
    "        targets = self.model.predict(states)\n",
    "        q_futures = self.model.predict(next_states).max(axis=1)\n",
    "        targets[(np.arange(self.batch_size), actions)] = rewards * dones + (rewards + q_futures*self.gamma)*undones\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0)\n",
    "        \n",
    "    def _run_one_episode(self, initial_state):\n",
    "        total_reward = 0\n",
    "        current_state = initial_state\n",
    "        for i in range(self.episode_length):\n",
    "            action = self.choose_action(current_state)\n",
    "            next_state, reward, done, _ = self._env.step(action)\n",
    "            next_state = next_state.reshape(1, 2)\n",
    "            reward += self.gamma * (next_state[0][1] - current_state[0][1])**2\n",
    "            self._memorize(current_state, action, reward, next_state, done)\n",
    "            self._replay()\n",
    "            total_reward += reward\n",
    "            current_state = next_state\n",
    "            if done:\n",
    "                break\n",
    "#         for i in range(5):\n",
    "#             self._replay()\n",
    "\n",
    "#         self._clear_memory()\n",
    "        self.epsilon -= self.epsilon_step\n",
    "        return total_reward\n",
    "    def train(self, episodes):\n",
    "        log = []\n",
    "        for episode in tqdm(range(episodes)):\n",
    "            initial_state = env.reset().reshape(1, 2)\n",
    "            log.append(self._run_one_episode(initial_state))\n",
    "        return log\n",
    "    \n",
    "    def test_one(self, render=False):\n",
    "        state = self.env.reset()\n",
    "        state = state.reshape(1, 2)\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            self.env.render() if render else 0\n",
    "            action = self.choose_action(state)\n",
    "            step += 1\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            state = next_state\n",
    "\n",
    "            # end if solved\n",
    "            if done and step < 200:\n",
    "                print(\"Climbed in {} steps\".format(step))\n",
    "                return 0\n",
    "        print(\"Task failed\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf629ba88ebb466c809c88ef3412b1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-199.99983905928005,\n",
       " -199.99985944440914,\n",
       " -199.99984760895865,\n",
       " -199.99992012956255,\n",
       " -199.99988791203614,\n",
       " -199.9998789651441,\n",
       " -199.99991412796257,\n",
       " -199.99990945841412,\n",
       " -199.99990376423492,\n",
       " -199.99981711749672,\n",
       " -199.99976498357503,\n",
       " -199.99984388368208,\n",
       " -199.9997737826617,\n",
       " -199.99981897514945,\n",
       " -199.99993024723233,\n",
       " -199.99978534387296,\n",
       " -199.99976532765993,\n",
       " -199.99996342638937,\n",
       " -199.99976755009698,\n",
       " -199.99994957598105]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coach = Coach(env)\n",
    "coach.train(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in 199 steps\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "\n",
    "for t_step in range(200):\n",
    "    env.render()\n",
    "    state = state.reshape(1, 2)\n",
    "    action = coach.choose_action(state)\n",
    "    \n",
    "    state, _, done, info = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"Finished in {} steps\".format(t_step))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: brute_training/assets\n"
     ]
    }
   ],
   "source": [
    "coach.model.save(\"brute_training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, alpha, gamma, epsilon, epsilon_interval, saving_path = \"./q-table\"):\n",
    "        self._env = gym.make('MountainCar-v0')\n",
    "        self.action_space = self.env.action_space.n  # 0 is push left, 1 is  no push and 2 is push right\n",
    "        self.observation_space = self.env.observation_space  # [x, v]; x \\in [-1.2; 0.6]; v \\in [-0.07, 0.07]\n",
    "\n",
    "        self.alpha = alpha  # learning rate\n",
    "        self.gamma = gamma  # discount rate\n",
    "        self.epsilon = epsilon  # probability of choosing a random action\n",
    "        self.epsilon_interval = epsilon_interval # change in epsilon between episodes\n",
    "\n",
    "        self.v_states = np.linspace(-0.07, 0.07, num=20)\n",
    "        self.x_states = np.linspace(-1.2, 0.6, num=20)\n",
    "        self.states_size = len(self.v_states) * len(self.x_states)\n",
    "        self.Q = np.zeros([self.states_size, self.action_space])\n",
    "        self.saving_path = saving_path\n",
    "    \n",
    "    def _get_Q_index(self, state):\n",
    "        i = np.searchsorted(self.x_states, state[0], side=\"left\")\n",
    "        j = np.searchsorted(self.v_states, state[1], side=\"left\")\n",
    "        return len(self.v_states) * i + j\n",
    "    \n",
    "    def save_Q_table(self, path):\n",
    "        return np.save(path + \".npy\", self.Q)\n",
    "    \n",
    "    def load_Q_table(self, path):\n",
    "        self.Q = np.load(path + \".npy\")\n",
    "        return 0\n",
    "        \n",
    "   \n",
    "    def train(self, episodes, load_old):\n",
    "        if load_old: \n",
    "            self.load_Q_table(self.saving_path)\n",
    "        print(\"Learning MountainCar-v0 model with {} episodes \".format(episodes))\n",
    "\n",
    "        global_max_score = -1e10\n",
    "        global_max_height = -1e10\n",
    "        episodes_to_solve = 0\n",
    "        self.env.seed(0)\n",
    "        scores = []\n",
    "        for i in range(1, episodes):\n",
    "            obs = self.env.reset()\n",
    "            state = self.get_Q_index(obs)\n",
    "            done = False\n",
    "            total_score = 0\n",
    "            max_height = -1e10\n",
    "            step = 0\n",
    "            while not done:\n",
    "                step += 1\n",
    "                if random.uniform(0, 1) < self.epsilon:  # e-greedy policy\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(self.Q[state])\n",
    "\n",
    "                next_obs, reward, done, info = self.env.step(action)\n",
    "                modified_reward = reward + self.gamma * abs(next_obs[1]) - abs(obs[1])  # reward based on potentials\n",
    "                next_state = self.get_Q_index(next_obs)\n",
    "\n",
    "                # update Q\n",
    "                self.Q[state, action] = (1 - self.alpha) * self.Q[state, action] + self.alpha * (\n",
    "                        modified_reward + self.gamma * np.max(self.Q[next_state]) - self.Q[state, action])\n",
    "                state = next_state\n",
    "\n",
    "                total_score += reward\n",
    "                max_height = max(max_height, next_obs[0])\n",
    "\n",
    "                # end if solved\n",
    "                if done and step < 200:\n",
    "                    if not episodes_to_solve:\n",
    "                        episodes_to_solve = i\n",
    "            scores.append(total_score)\n",
    "            self.epsilon -= 5 * self.epsilon / episodes if self.epsilon > 0 else 0  # epsilon reduction\n",
    "            self.epislon = max(0, self.epsilon)  # epsilon reduction\n",
    "            global_max_score = max(global_max_score, total_score)\n",
    "            global_max_height = max(global_max_height, max_height)\n",
    "            if i % 5 == 0:\n",
    "                print(\"Episode: {}\".format(i))\n",
    "                print(\" Total score for episode {} : {}, Max height : {}\".format(i, total_score, max_height))\n",
    "                print(\" GLOBAL MAXIMUMS: Max score : {}, Max height  : {}\".format(global_max_score, global_max_height))\n",
    "                print('-' * 150)\n",
    "                self.save_Q_table(self.saving_path)\n",
    "\n",
    "        print(\"Training finished\\n\")\n",
    "        solve_status = \"Solved in {} episodes\".format(episodes_to_solve) if global_max_height >= 0.5 else \"Not Solved\"\n",
    "        print(\"Max score: {} , Max height: {}, Solve status : {}\".format(global_max_score, \n",
    "                                                                                    global_max_height,\n",
    "                                                                                    solve_status))  \n",
    "        \n",
    "    def test_one(self, render=False):\n",
    "        obs = self.env.reset()\n",
    "        state = self.get_Q_index(obs)\n",
    "        done = False\n",
    "        step = 0\n",
    "        while not done:\n",
    "            self.env.render() if render else 0\n",
    "            action = np.argmax(self.Q[state])\n",
    "            step += 1\n",
    "            next_obs, reward, done, info = self.env.step(action)\n",
    "            next_state = self.get_Q_index(next_obs)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            # end if solved\n",
    "            if done and step < 200:\n",
    "                print(\"Climbed in {} steps\".format(step))\n",
    "                return 0\n",
    "        print(\"Task failed\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning MountainCar-v0 model with 100 episodes \n",
      "Episode: 5\n",
      " Total score for episode 5 : -200.0, Max height : 0.05188456600002128\n",
      " GLOBAL MAXIMUMS: Max score : -200.0, Max height  : 0.05188456600002128\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 10\n",
      " Total score for episode 10 : -200.0, Max height : -0.026202213836258717\n",
      " GLOBAL MAXIMUMS: Max score : -200.0, Max height  : 0.11136836246413265\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 15\n",
      " Total score for episode 15 : -165.0, Max height : 0.5012204046240831\n",
      " GLOBAL MAXIMUMS: Max score : -165.0, Max height  : 0.5012204046240831\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 20\n",
      " Total score for episode 20 : -169.0, Max height : 0.5180010143185333\n",
      " GLOBAL MAXIMUMS: Max score : -150.0, Max height  : 0.5180010143185333\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 25\n",
      " Total score for episode 25 : -147.0, Max height : 0.5000408663140371\n",
      " GLOBAL MAXIMUMS: Max score : -147.0, Max height  : 0.5359177218033268\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 30\n",
      " Total score for episode 30 : -144.0, Max height : 0.5091528635318807\n",
      " GLOBAL MAXIMUMS: Max score : -144.0, Max height  : 0.5359177218033268\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 35\n",
      " Total score for episode 35 : -176.0, Max height : 0.520692371985102\n",
      " GLOBAL MAXIMUMS: Max score : -144.0, Max height  : 0.53858548053072\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 40\n",
      " Total score for episode 40 : -200.0, Max height : 0.33236912160707305\n",
      " GLOBAL MAXIMUMS: Max score : -125.0, Max height  : 0.53858548053072\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 45\n",
      " Total score for episode 45 : -155.0, Max height : 0.5247781764897423\n",
      " GLOBAL MAXIMUMS: Max score : -125.0, Max height  : 0.53858548053072\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 50\n",
      " Total score for episode 50 : -156.0, Max height : 0.5135725397924035\n",
      " GLOBAL MAXIMUMS: Max score : -120.0, Max height  : 0.53858548053072\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 55\n",
      " Total score for episode 55 : -185.0, Max height : 0.5366223984290598\n",
      " GLOBAL MAXIMUMS: Max score : -113.0, Max height  : 0.53858548053072\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 60\n",
      " Total score for episode 60 : -116.0, Max height : 0.5034569712615579\n",
      " GLOBAL MAXIMUMS: Max score : -113.0, Max height  : 0.53858548053072\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 65\n",
      " Total score for episode 65 : -184.0, Max height : 0.5245053063429725\n",
      " GLOBAL MAXIMUMS: Max score : -113.0, Max height  : 0.5446163986773849\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 70\n",
      " Total score for episode 70 : -151.0, Max height : 0.5070418302553329\n",
      " GLOBAL MAXIMUMS: Max score : -113.0, Max height  : 0.5446163986773849\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 75\n",
      " Total score for episode 75 : -149.0, Max height : 0.5271974114825816\n",
      " GLOBAL MAXIMUMS: Max score : -113.0, Max height  : 0.5446163986773849\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 80\n",
      " Total score for episode 80 : -149.0, Max height : 0.5070418302553329\n",
      " GLOBAL MAXIMUMS: Max score : -113.0, Max height  : 0.5446163986773849\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 85\n",
      " Total score for episode 85 : -159.0, Max height : 0.5070418302553329\n",
      " GLOBAL MAXIMUMS: Max score : -113.0, Max height  : 0.5446163986773849\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 90\n",
      " Total score for episode 90 : -150.0, Max height : 0.512155252152702\n",
      " GLOBAL MAXIMUMS: Max score : -113.0, Max height  : 0.5446163986773849\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Episode: 95\n",
      " Total score for episode 95 : -143.0, Max height : 0.5111130606593716\n",
      " GLOBAL MAXIMUMS: Max score : -113.0, Max height  : 0.5446163986773849\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Training finished\n",
      "\n",
      "Max score: -113.0 , Max height: 0.5446163986773849, Solve status : Solved in 15 episodes\n",
      "Climbed in 142 steps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_number = 100  # number of episodes\n",
    "learn_until_solved = False  # stop if solved\n",
    "rendering = False  # picture\n",
    "epsilon = 0.1  # probability of choosing a random action\n",
    "alpha = 0.5  # learning rate\n",
    "gamma = 0.8  # discount rate\n",
    "agent = QAgent(epsilon, alpha, gamma, 0.01)\n",
    "agent.train(episode_number, 1)\n",
    "agent.test_one(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
